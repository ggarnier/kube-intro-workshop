Kubernetes Intro Workshop

Guilherme Garnier

* Requirements
.html images/style.html

This presentation and the code we are going to use is available at:

[[https://github.com/ggarnier/kube-intro-workshop]]
[[https://go-talks.appspot.com/github.com/ggarnier/kube-intro-workshop/workshop.slide]]

* Setup

- Kubectl - CLI for interacting with the Kubernetes cluster

[[https://kubernetes.io/docs/tasks/tools/install-kubectl/]]

- Minikube or Microk8s - to run a local Kubernetes cluster

[[https://github.com/kubernetes/minikube#installation]]
[[https://microk8s.io/]]

- Katacoda, Play with Kubernetes - playgrounds
[[https://www.katacoda.com/courses/kubernetes/playground]]
[[https://labs.play-with-k8s.com/]]

* Learning

- Official docs
[[https://kubernetes.io/docs/home/]]

- "Kubernetes in Action" book
[[https://www.manning.com/books/kubernetes-in-action]]

* History

*2003-2004:* Birth of *Borg* inside Google
[[https://research.google/pubs/pub43438/]]

*2013:* *Borg* -> *Omega*
[[https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf]]

*2014:* *Kubernetes* released
[[https://cloudplatform.googleblog.com/2014/06/an-update-on-container-support-on-google-cloud-platform.html]]

* What is Kubernetes

When people hear about kubernetes one of the first definitions that come to
mind is:

"Kubernetes is a container orchestrator."

It knows:

- *How* to start containers
- *When* to start containers
- *Where* to start containers

: One of the official definitions is "Kubernetes is an open-source system for
: automating deployment, scaling, and management of containerized applications."
: It does a lot of things not directly related to containers but containers are
: the core functionality.

: Another more broad definition is that Kubernetes is a platform for
: distributed applications.

: Ask if the audience is familiar with the concept of plain docker containers
: and basic explanation if needed.

* Architecture overview

Resources created on the API describe the *desired* state.

The Control Plane is responsible for making the *desired* state a reality.

.image images/kubernetes-control-plane.png _ 700
.caption By [[https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g1e639c415b_0_56][Lucas Käldström]]

* Concepts

Kubernetes provides an API for CRUDing objects.

Basic objects:

- Pods
- Namespaces
- Services
- Endpoints
- Volumes
- ConfigMap
- Nodes

The *control* *plane* is responsible for storing these objects and doing something with them.

* kubectl

  kubectl get <resource>
    -o wide / yaml / json
    -l app=myapp,env=staging
    --field-selector status.phase=ContainerCreating
    -n mynamespace / -A (= --all-namespaces)
    --show-labels
    -L app,env
    -w (= --watch)

  # verbose mode, useful for debugging
  kubectl -v=8 get <resource>

  # documentation for resource fields
  kubectl explain pod.spec.containers

  # create/update resources
  kubectl create/apply -f file.yaml

  # update resource using your default editor
  kubectl edit <resource> <name>

More info: [[https://kubernetes.io/docs/reference/kubectl/cheatsheet/]]

* YAML

Usually YAML files are used to represent a Kubernetes object, example:

  apiVersion: v1
  kind: Pod

  metadata:
    name: mypod
    namespace: lab
    labels:
    annotations:

  spec:
    containers:
      - name: mypod1
        image: busybox

  status:

These 4 blocks are common for almost all Kubernetes objects.

* Pods

* Pods

Similar to docker containers, except one pod can contain more than one container.

Containers in the same pod:
- Are scheduled on the same node;
- Share the Network namespace (same IP);
- Share volumes assigned to the Pod.

.code labs/pod.yaml

: PID namespace can also optionally be shared as of 1.10.

* Hands on: kubectl

- Bootstrap minikube and check cluster

  $ minikube start
  $ kubectl cluster-info

- Manipulating resources with kubectl

  $ kubectl create -f ./labs/pod.yaml
  $ kubectl get pod

  # What more can we do with pods?
  $ kubectl explain pod.spec

* Pods - what happened?
  -
                    +------+
                    | etcd |
                    +---^--+
                        |
                    +------------------------------------+
                    |   |2                               |
                    |  +------------+3     +-----------+ |
    +---------+ 1   |  |            +------>           | |
    |         +--------> api-server |     4| scheduler | |
    | kubectl |     |  |            <------+           | |
    |         |     |  +------^-----+      +-----------+ |
    +---------+     |   |5    |                          |
                    +------------------------------------+
                        |     |
                    +------------------------------------+
                    |   |     |6                         |
                    |  +v--------+                       |
                    |  |         |                       |
                    |  | kubelet |                       |
                    |  |         |                       |
                    |  +---------+                       |
                    +------------------------------------+

* Pods - what happened?
  1 - kubectl    -> api-server:
    I want to create a Pod.

  2 - api-server -> etcd:
    Hey etcd, please store this new pod.

  3 - api-server -> scheduler:
    Hi scheduler, you asked to be notified on new pods, here is a new one.

  4 - scheduler  -> api-server:
    Thanks, I can see this pod has no Node yet, I've chosen one and updated this Pod.

  5 - api-server -> kubelet:
    Hey kubelet, you asked to be notified when a pod is updated, here is this one.

  6 - kubelet    -> api-server:
    Oh thanks, I can see this pod was supposed to be running on my node.
    I've just started it and updated the Pod status.

* Pod with 2 containers

.code labs/pod-with-two-containers.yaml /START OMIT/,/END OMIT/

* ConfigMap

Config maps are objects used to store configuration and make this configuration available for pods to use.

.code labs/configmap.yaml

* ConfigMap

Using config maps inside a pod by mounting the config map inside the filesystem.

.code labs/podconfigmap.yaml

* ConfigMap

Using config maps as env vars

.code labs/podconfigmap-env.yaml

* Namespaces

Namespaces can be thought as a way to divide the cluster in multiple sub-clusters.

They are intended to be used when working with multiple team or projects.

By default every cluster is born with 2 important namespaces:

- default
- kube-system

* Hands on: Where is this control-plane?

  $ kubectl get namespaces
  $ kubectl -n kube-system get pod

  $ # create a new namespace, similar to pod creation

* Service and Endpoint

- Pods may vanish anytime their IP address will also vanish;

- Services provide a stable IP and Name to find applications on Kubernetes;

.code labs/service.yaml

* Selectors

.image images/selectors.png _ 800

* Service and Endpoint

- The selector is very important here, it selects which pods belong to this service based on Pod labels;

- For each pod that matches the selector kubernetes will create an Endpoint object;

- An endpoint is only created for healthy Pod IPs;

- In the control plane, the `kube-proxy` is responsible for ensuring a packet destined to a service IP is correctly forwarded to a pod IP.

* Hands on: kubectl

- Bootstrap minikube and check cluster

  $ minikube start
  $ kubectl cluster-info

- Manipulating resources with kubectl

  $ kubectl get pod -o wide
  $ kubectl create -f ./labs/service.yaml
  $ kubectl get service
  $ minikube service list
  $ kubectl get endpoints

: Explain why minikube is used (only to get node IP)

* Networking

Kubernetes uses Container Networking Interface (CNI) plugins for Networking. There are many plugable implementations.

Any implementation must adhere to the following requirements:

- All containers can communicate with all other containers without NAT;
- All nodes can communicate with all containers (and vice-versa) without NAT;
- The IP that a container sees itself as is the same IP that others see it as.

* Controller objects

* Controller objects

- Deployment;
- ReplicaSet;
- DaemonSet;
- StatefulSet.

* What do controllers do

- Control loop that listen for changes and do something;

- Dinamically create or manipulate basic objects;

- Built-in controllers are implemented in the `controller-manager` component.

- The scheduler is also a controller:
-    Watch for created pods;
-    Assigns a node to the pod.

- The kubelet daemon is also a controller:
-    Watch for pods with a node assigned;
-    Start new containers on the container runtime (usually docker).

* Pods - Limitations

- Pods will die, on their own they do not provide resilience;

- A pod scheduled to a node will always run on the same node. There's no such thing as a _rescheduled_ pod;

- A higher level abstraction is needed to provide resilience;

- Only a very limited set of attributes can be modified on exiting pods.

* Deployment

.code labs/deployment.yaml

* Deployment

Important sections:

  template:
    ...

The template section represents the same objects as the `pod.metadata` and `pod.spec` fields. The name is not required as it'll be based on the deployment name.

  selector:
    matchLabels:
      app: nginx

The selector *must* be a subset of the pod labels. It'll let the controller know which Pods belong to this deployment.

* Hands on: Create deployment with kubectl

  $ kubectl create -f ./labs/deployment.yaml
  $ kubectl get pod,deployment,replicaset

- Deployments manage the pod lifecycle;
- Ensure the number of replica pods exist;
- If a node is down a new pod will be created to replace the missing one;
- Manages rolling update of new pod templates.

* ReplicaSet

- Does most of the work for the deployment controller, except rolling updates;
- It's not usually created manually.

* What does the controller do

Deployment controller

- Watches for deployment object to appear;
- Creates a new replicaSet object every time the template field is changed;
- Updates the old replicaSet replicas count to zero.

ReplicaSet controller

- Watches for replica set objects;
- Create new pods matching the number of replica pods;
- Watches for pods matching the selector labels;
- If a pod dies creates a new one to replace it.

: Clarify this is an overly simplified explanation

* Updating Deployments

.image images/deployment-v1.png _ 500

* Updating Deployments

.image images/deployment-v2.png _ 900

* Rolling updates

Deployments ensure that a number of replica pods are always available.
We can even try to manually destroy them and they will eventually be back.

Rolling update allow us to configure how deployments are updated ensuring it never becomes unavailable

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 1

* Hands on: Rolling updates

  $ kubectl apply -f ./labs/deployment-rolling.yaml
  $ kubectl rollout status deployment/rolling-deployment
  $ ... change image version to 1.9.1 ...
  $ kubectl apply -f ./labs/deployment-rolling.yaml
  $ kubectl rollout status deployment/rolling-deployment

* Pod topics

* Pod topics

- Requests/Limits
- QoS Class
- Liveness/Readiness probes
- Restart policy
- Affinity

* Resource requests and limits

Requests and Limits are the kubernetes way of limits resource usage by containers.

  ...
  resources:
    requests:
      memory: "64Mi"
      cpu: "250m"
    limits:
      memory: "128Mi"
      cpu: "500m"
  ...

`requests` represent the requested value for the resource, it will be used by the scheduler to choose a node where the requested value is available.

`limits` represent the absolute limit for using the specified resource. In the case of memory, going over this threshold will cause the kernel OOM to kill a process in the pod.

* QoS Class

The concept of QoS classes is directly related to resource requests and limits.

The QoS class is selected automatically by Kubernetes and helps deciding what to do when a node is running out of resources.

- Guaranteed
The `requests` value must be set and must equal the `limits` value on all containers for both cpu and memory.

- Burstable
At least one container in the pod has either memory or cpu `requests` value set.

- BestEffort
No `requests` and no `limits` on any container.

* Hands on: QoS class

  $ kubectl apply -f labs/pod-resource.yaml
  $ kubectl get pod mylimitedpod -o yaml

Modify the pod to match Guaranteed criteria.
Stress the node to see eviction happening.

* Liveness/Readiness probes

Liveness and readiness probes are ways for Kubernetes to verify if a container is running correctly and take some action based on this knowledge.

Both probes are performed continuously by kubelet during the lifecycle of a pod.

- Readiness

A failing readiness probe causes kubernetes to remove the endpoint for the Pod IP from matching service. The pod is marked as unready and no other action is taken.

- Liveness

A failing liveness probe will make kubernetes kill the Pod. Depending on the restart policy configuration it will be restarted or not.

* Probe types

There are three types of probes:

- *httpGet*

An HTTP GET request, expects status code 2xx or 3xx

- *tcpSocket*

Performs a TCP check, expects an open port

- *exec*

Runs a custom command in the container, expects exit status 0

* Example

  readinessProbe:
    httpGet:
      path: /health
      port: 8080
    successThreshold: 2  # default: 1
    timeoutSeconds: 2  # default: 1
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 20  # default: 10
    failureThreshold: 5  # default: 3

* Hands on: Probes

  $ kubectl apply -f labs/pod-probe.yaml
  $ kubectl get pod --watch

  $ kubectl explain pod.spec.containers.livenessProbe.exec
  $ kubectl explain pod.spec.containers.livenessProbe.tcpSocket
  $ kubectl explain pod.spec.containers.livenessProbe.httpGet

* Let's talk about nodes

* Nodes

Nodes are also registered as kubernetes objects.
The kubelet process on each node is responsible for updating the node status and checking its resource usage.

* Hands on: Nodes

  $ kubectl get nodes
  $ kubectl describe nodes

Nodes as any other object can have metadata as labels and annotations, and some are added automatically.

: Talk about describe

* Affinity and anti-affinity

Affinities are ways to interact with the scheduler and and change how it's going to work.

With affinities it's possible to say that a pod should only be scheduled on nodes with some specific label.
On the other hand, it's also possible to say that a pod should *not* be scheduled on a node containting a certain kind of pod.

* Node Affinity

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:              # OR
          - key: beta.kubernetes.io/arch # AND
            operator: In
            values:
            - amd64

- requiredDuringSchedulingIgnoredDuringExecution (equivalent to `nodeSelector`)
- preferredDuringSchedulingIgnoredDuringExecution

Also:
- podAffinity
- podAntiAffinity

* Hands on: Node Affinity

Let's modify a pod to add a node affinity to it

* Taints and Tolerations

Nodes can have `taints`, taints are a way for telling the scheduler that this node should not be used by a pod unless the pod `tollerates` the specified taint.

Using this mechanism we can for example make master nodes unschedulable.

It's also useful for debugging a node, no pods will use it except pods made to tollerate it while debugging.

* Taints and Tolerations

In nodes:

  taints:
    - effect: NoSchedule
      key: mytaint
      value: myvalue

In pods:

  tolerations:
    - key: mytaint
      operator: Exists

Effects can be:

- NoSchedule
- PreferNoSchedule
- NoExecute (force eviction)

* Hands on: Taints/Tolerations

  $ kubectl taint node minikube mytaint=myvalue:NoSchedule
  $ kubectl apply -f ./labs/pod-toleration.yaml
  $ ... error ...
  $ ... modify the pod to add a toleration ...
  $ kubectl apply -f ./labs/pod-toleration.yaml

* Service topics

* Service topics

- Service discovery
- Service types
- Service ports
- Headless services

* Service discovery

A kubernetes cluster includes its own dns server. It can either be known as `kube-dns` (based on dnsmasq) or `CoreDNS` (new DNS server implementation in Go).

The default resolver for any created pod points to the internal DNS server and it is responsible for also forwarding DNS queries to outside the cluster.

An internal DNS entry is created for each service usually as:

  <service name>.<namespace>.svc.cluster.local A <Service Cluster IP>

* Hands on: DNS entries

Based on previous hands on exercises our cluster should have 2 running services, `nginx-sidecar` and `myservice`.

  $ kubectl get services
  $ kubectl run --rm -it ubuntu --image=ubuntu --restart=Never
  $ ... inside pod ...
  $ apt-get update && apt-get install curl -y
  $ curl nginx-sidecar
  $ curl nginx-sidecar.default.svc.cluster.local
  $ curl <service ip>
  $ cat /etc/resolv.conf

* Service types

- ClusterIP
Is the default type of service, a cluster-wide IP address is allocated to the Service. DNS entries are created pointing to this IP address. It's only accessible from inside the cluster.

- NodePort
Extends the ClusterIP type by also allocating a port on the *node* IP for each service port. Traffic to the node IP on this port is then forwarded to the cluster IP on the service port.
This makes a service available from outside the cluster. We've been using this type of service with minikube when using its IP.

* Service types

- LoadBalancer
Extends the NodePort type by also creating a load balancer IP on the *cloud* *provider* being used.
Cloud providers are the bridge between Kubernetes and a cloud like AWS, Google Cloud or Cloudstack.
Kubernetes itself does nothing for LoadBalancer services and a external controller known as *cloud* *controller* *manager* must be installed in the cluster for LoadBalancer services to work.
On AWS a ELB IP may be created for a LoadBalancer service.
On Cloudstack a ACS LoadBalancerRule is created.

- ExternalName
External name services are a bit different from the rest as they simply create a CNAME record in the internal DNS server pointing to the `service.spec.externalName` field.

* Headless services

Headless services are a special kind of ClusterIP service where the ClusterIP value is explicitly set to `None`.

Because a cluster-wide IP address is not created the A DNS entry for the service returns the IP address of each individual pod IP.

* DaemonSets

* DaemonSets

Daemon sets are a different kind of controller managed object.

They are useful when we want to ensure the same pod exists on every Node - or nodes that match `spec.template.spec.nodeSelector` or `spec.template.spec.affinity`.

Usage examples are log processing, node metrics collection, node networking configuration.

* DaemonSets

  apiVersion: apps/v1
  kind: DaemonSet
  spec:
    template:
      spec:
        nodeSelector:
          disktype: ssd

Object specification similar to Deployments

* Hands-on: DaemonSet

  $ kubectl apply -f ./labs/daemonset.yaml
  $ kubectl get daemonset
  $ kubectl get pods

* StatefulSets

* StatefulSets

StatefulSets work similarly to Deployments, but they provide a stable identity to Pods - their identity is kept even if the Pods are rescheduled to other Nodes.

This is useful to run applications like databases, that require stable network identifiers and persistent storage.

* StatefulSets

Each StatefulSet need a previously created Service, to be responsible for the network identify of the Pods:

  apiVersion: v1
  kind: Service
  metadata:
    name: mongodb
    labels:
      namespace: lab
      name: mongodb
  spec:
    ports:
    - port: 27017
      targetPort: 27017
    selector:
      name: mongodb

* StatefulSets

  apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: mongodb
  spec:
    serviceName: mongodb
    replicas: 3
    template:
      metadata:
        labels:
          name: mongodb
      spec:
        terminationGracePeriodSeconds: 10
        containers:
          - name: mongodb
            image: mongo
            command:
              - mongod
              - "--replSet"
              - rs0
            ports:
              - containerPort: 27017
            volumeMounts:
              # ...

* Secrets

* Secrets

Secrets are much like ConfigMaps, but they let you store sensitive information

  $ cat data.env
  username=admin
  password=s3cr3t0
  $ kubectl create secret generic db-user-pass \
        --from-env-file=./data.env

  # decoding
  $ kubectl get secret db-user-pass -o yaml
  data:
    password: czNjcjN0MA==
    username: YWRtaW4=
  ...
  $ echo "YWRtaW4=" | base64 --decode
  admin

* Secrets

Using a secret on a pod: mounting a volume

.code labs/pod-secret-volume.yaml

* Secrets

Using a secret on a pod: setting env vars

.code labs/pod-secret-env.yaml

* Volumes

* Volumes

Volumes allow exposing external volumes to containers.

External volumes can be:

- Directories on the node;
- Network drives like NFS, or Ceph Block Device;
- Cloud based block devices like EBS.

* Volumes

  volumes:
  - name: shared-data
    nfs:
      path: /exported/path
      server: 10.0.0.1
  ...
  containers:
  - name: cont1
    volumeMounts:
      name: shared-data
      mountPath: /mnt/dir

* Jobs

* Jobs

A Job creates one or more Pods and ensures that a specified number of them successfully terminate. The Job completes when a specified number of successful completions is reached.

You can use a Job to run multiple Pods in parallel.

* Jobs

.code labs/job.yaml

* CronJobs

* CronJobs

Allow running a task on a time-based schedule.

One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format.

* CronJobs

Running a command every minute (`*/1`)

.code labs/cronjob.yaml

* Much, much more

- RBAC
- PersistentVolume
- PersistentVolumeClaim
- StorageClass
- NetworkPolicy
- ResourceQuota
- ...
